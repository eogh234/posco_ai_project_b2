{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f979a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd45b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-345b76b2e64e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m              \u001b[0mpt_vtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m              \u001b[0mpt_vtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m              \u001b[0mpt_vtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        pc = rs.pointcloud()\n",
    "        points = rs.points\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth = frames.get_depth_frame()\n",
    "        color = frames.get_color_frame()\n",
    "        pc.map_to(color)\n",
    "        points = pc.calculate(depth)\n",
    "        color_img = np.asanyarray(color.get_data())\n",
    "        depth_img = np.asanyarray(depth.get_data())\n",
    "        vtx = np.asanyarray(points.get_vertices())\n",
    "        tex = np.asanyarray(points.get_texture_coordinates())\n",
    "        pt_vtx = np.zeros( (len(vtx), 3) , float )\n",
    "        for i in range(len(vtx)):\n",
    "             pt_vtx[i][0] = np.float(vtx[i][0])\n",
    "             pt_vtx[i][1] = np.float(vtx[i][1])\n",
    "             pt_vtx[i][2] = np.float(vtx[i][2])\n",
    "\n",
    "    # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "    if depth_colormap_dim != color_colormap_dim:\n",
    "        resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "        images = np.hstack((resized_color_image, depth_colormap))\n",
    "    else:\n",
    "        images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "    # Show images\n",
    "    cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.imshow('RealSense', images)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a3517",
   "metadata": {},
   "source": [
    "# 휴스타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee61b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9919, 1)\n",
      "(8704, 1)\n",
      "(8739, 1)\n",
      "(8689, 1)\n",
      "(8740, 1)\n",
      "(8734, 1)\n",
      "(8237, 1)\n",
      "(7849, 1)\n",
      "(8577, 1)\n",
      "(8602, 1)\n",
      "(8712, 1)\n",
      "(8837, 1)\n",
      "(9102, 1)\n",
      "(8937, 1)\n",
      "(8907, 1)\n",
      "(8727, 1)\n",
      "(8842, 1)\n",
      "(9302, 1)\n",
      "(9245, 1)\n",
      "(9255, 1)\n",
      "(8949, 1)\n",
      "(8989, 1)\n",
      "(9026, 1)\n",
      "(9417, 1)\n",
      "(9403, 1)\n",
      "(9502, 1)\n",
      "(9441, 1)\n",
      "(9202, 1)\n",
      "(9322, 1)\n",
      "(9332, 1)\n",
      "(9477, 1)\n",
      "(9062, 1)\n",
      "(9030, 1)\n",
      "(8991, 1)\n",
      "(9025, 1)\n",
      "(9040, 1)\n",
      "(9024, 1)\n",
      "(8995, 1)\n",
      "(9000, 1)\n",
      "(9020, 1)\n",
      "(9040, 1)\n",
      "(9008, 1)\n",
      "(8990, 1)\n",
      "(8960, 1)\n",
      "(8975, 1)\n",
      "(9023, 1)\n",
      "(9002, 1)\n",
      "(9017, 1)\n",
      "(8964, 1)\n",
      "(9044, 1)\n",
      "(9016, 1)\n",
      "(9010, 1)\n",
      "(9022, 1)\n",
      "(9030, 1)\n",
      "(9002, 1)\n",
      "(9035, 1)\n",
      "(8996, 1)\n",
      "(9031, 1)\n",
      "(9033, 1)\n",
      "(9045, 1)\n",
      "(9086, 1)\n",
      "(9080, 1)\n",
      "(9071, 1)\n",
      "(9052, 1)\n",
      "(9106, 1)\n",
      "(9103, 1)\n",
      "(9044, 1)\n",
      "(9061, 1)\n",
      "(9094, 1)\n",
      "(9029, 1)\n",
      "(9043, 1)\n",
      "(9095, 1)\n",
      "(9056, 1)\n",
      "(9045, 1)\n",
      "(9021, 1)\n",
      "(9024, 1)\n",
      "(8965, 1)\n",
      "(8897, 1)\n",
      "(8746, 1)\n",
      "(8450, 1)\n",
      "(8802, 1)\n",
      "(9035, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1dbaefbfe2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcolor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcolor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)\n",
    "\n",
    "pipeline.start(config)\n",
    "\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n",
    "try:\n",
    "    while True:\n",
    "    # 비디오의 한 프레임씩 읽는다.\n",
    "    # 제대로 읽으면 ret = True, 실패면 ret = False, frame에는 읽은 프레임\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.resize(color_image,dsize=(320,240),interpolation=cv2.INTER_AREA)\n",
    "        result, color_frame = cv2.imencode('.jpg', color_image, encode_param)\n",
    "        print(color_frame.shape)\n",
    "        \n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', color_image)\n",
    "#         cv2.imshow('RealSense', color_frame)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "finally:\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff9ccce",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/imgproc/src/resize.cpp:4054: error: (-215:Assertion failed) inv_scale_x > 0 in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ba2c77137541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcolor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mframe_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mframe_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mframe_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/imgproc/src/resize.cpp:4054: error: (-215:Assertion failed) inv_scale_x > 0 in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "# Video_capture\n",
    "\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "frame_queue = None\n",
    "width = None\n",
    "height = None\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)\n",
    "\n",
    "pipeline.start(config)\n",
    "\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n",
    "\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    if not color_frame:\n",
    "        continue\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    frame_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    frame_queue.put(frame_resized)\n",
    "    print(frame_queue)\n",
    "\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c562d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth Scale is:  0.0010000000474974513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-adad9140aa51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# 비디오의 한 프레임씩 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mdepth_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_depth_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#         dpt_frame = depth_frame.as_depth_frame()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 휴스타\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)\n",
    "    \n",
    "    \n",
    "# config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8)  \n",
    "                    # bgr8: 8비트 blue, green, red 채널로 구성된 픽셀  / OpenCv에 적합한 형식\n",
    "   \n",
    " # Start Streaming\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Depth 계산하기!!!!!!!!!!!!!!!!!!!\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "print('Depth Scale is: ', depth_scale)\n",
    "\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n",
    "                # 90: 이미지 품질 설정. 0부터 100까지 가능하며 높을수록 품질이 좋음. default 95\n",
    "\n",
    "# rs2_deproject_pixel_to_point takes 3 arguments. (intrinsic, pixel, depth)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # 비디오의 한 프레임씩 읽기\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "#         dpt_frame = depth_frame.as_depth_frame()\n",
    "#         pixel_distance_in_meters = dpt_frame.get_distance(dpt_)\n",
    "        color_frame = frames.get_color_frame()\n",
    "    \n",
    "#         _intrinsics = rs.intrinsics()\n",
    "#         pixel = rs.\n",
    "        \n",
    "#         result = rs.rs2_deproject_pixel_to_point()\n",
    "#         x = result[2]\n",
    "#         y = -result[0]\n",
    "#         z = -resilt[1]\n",
    "#         print(x, y, z)\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "            \n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        # 깊이 구하기~!!!!!!!!!!!!\n",
    "        depth = depth_image[320, 240].astype(float)\n",
    "        distance = depth * depth_scale\n",
    "        \n",
    "        print('Distance (m) ', distance)\n",
    "        \n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "#         print(depth_colormap_dim)\n",
    "        color_colormap_dim = color_image.shape\n",
    "        \n",
    "        # encode_param의 형식으로 frame을 jpg로 인코딩\n",
    "        color_image = cv2.resize(color_image, dsize = (320, 420), interpolation = cv2.INTER_AREA)\n",
    "                        # INTERPOLATION 보간법 -> INTER_AREA: 영역 보간법\n",
    "#         result, color_frame = cv2.imencode('.jpg', color_image, encode_param)\n",
    "        \n",
    "#           Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', color_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        \n",
    "finally:\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5959f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a64ae21af62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Align the depth frame to color frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0maligned_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Get aligned frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "\n",
    "# Pointcloud persistency in case of dropped frames\n",
    "pc = rs.pointcloud()\n",
    "points = rs.points()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a config and configure the pipeline to stream\n",
    "config = rs.config()\n",
    "\n",
    "# This is the minimal recommended resolution for D435\n",
    "config.enable_stream(rs.stream.depth,  848, 480, rs.format.z16, 90)\n",
    "config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Getting the depth sensor's depth scale\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "\n",
    "# Create an align object\n",
    "align_to = rs.stream.depth\n",
    "\n",
    "align = rs.align(align_to)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        depth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "        # Tell pointcloud object to map to this color frame\n",
    "        pc.map_to(color_frame)\n",
    "\n",
    "        # Generate the pointcloud and texture mappings\n",
    "        points = pc.calculate(depth_frame)\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b58dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIBD",
   "language": "python",
   "name": "aibd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
